{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "(view) msc_project_bert_processing_final.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Will170393/MSc-Project---Stance-Detection/blob/master/(view)_msc_project_bert_processing_final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t49wBiJgU4WE",
        "colab_type": "text"
      },
      "source": [
        "**Importing libraries**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N5qekic7Ugoi",
        "colab_type": "code",
        "outputId": "e6564c01-aac1-4dfc-fde5-0a49a5f251a7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "# libraries for processing data\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "# libraries for loading files from drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "# libraries for natural language processing\n",
        "from nltk import FreqDist, word_tokenize\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "import torch\n",
        "from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hpVtvKXiWkgj",
        "colab_type": "text"
      },
      "source": [
        "**Reading data into panda dataframes and merging Stances and Bodies**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nzP9HuUQWbAi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# creates dataframes from reading the stance and body csv files from the google drive, train/test set already split\n",
        "trainStances = pd.read_csv('gdrive/My Drive/Colab Notebooks/MSC_project_data/train_stances.csv')\n",
        "trainBodies = pd.read_csv('gdrive/My Drive/Colab Notebooks/MSC_project_data/train_bodies.csv')\n",
        "testStances = pd.read_csv('gdrive/My Drive/Colab Notebooks/MSC_project_data/competition_test_stances.csv')\n",
        "testBodies = pd.read_csv('gdrive/My Drive/Colab Notebooks/MSC_project_data/competition_test_bodies.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "95zW1w-_We8j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# merges dataframes for article bodies and article stances, inner merge on the Body ID column that appears in both dataframes\n",
        "def mergeStances_Bodies(stances, bodies):\n",
        "  return pd.merge(stances, bodies, how='inner', on='Body ID')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OEpr_xbYWh97",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# creates merged training and test dataframes\n",
        "train = mergeStances_Bodies(trainStances, trainBodies)\n",
        "test = mergeStances_Bodies(testStances, testBodies)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fO9kv_8hem3l",
        "colab_type": "text"
      },
      "source": [
        "**Data Cleaning**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4_cTjgeneoEI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#removes non-alphabetic characters from strings and make all characters lower case\n",
        "def data_cleaning(data, col):\n",
        "  p = re.compile(r'[^\\w\\s]+')\n",
        "  data[col] = [p.sub('', str(string)) for string in data[col].tolist()] #uses regex to substitute all non_alphabtic characters with whitespace\n",
        "  data[col] = [[character.lower() for character in word_tokenize(string)] for string in data[col]] #tokenizes the words and converts characters to lower case\n",
        "  data[col] = [' '.join(word) for word in data[col]] "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LC4sJKkmerLA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# cleaning dataframes for article Headlines and Bodies\n",
        "data_cleaning(train, 'Headline')\n",
        "data_cleaning(train, 'articleBody')\n",
        "data_cleaning(test, 'Headline')\n",
        "data_cleaning(test, 'articleBody')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RjkpqapN7LRR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Load pre-defined stops words file from google drive into a list\n",
        "f = open('gdrive/My Drive/Colab Notebooks/MSC_project_data/project_stopwords_final.txt', encoding='utf-8-sig')\n",
        "stop_words = f.read().split('\\n')\n",
        "f.close"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MPXjEGdc7NzZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#removing stop words from training and test set\n",
        "def remove_stop_words(data, col, stop_words):\n",
        "  filtered_sentence = data[col].apply(lambda text: [word for word in text.split() if word not in stop_words]) #all all tokens not in stop words list to filtered_tokens\n",
        "  data[col] = [' '.join(word) for word in filtered_sentence]                                         "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MlbKlUpn7Qyu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# removing stopwords in dataframes for article Headlines and Bodies\n",
        "remove_stop_words(train, 'Headline', stop_words)\n",
        "remove_stop_words(train, 'articleBody', stop_words)\n",
        "remove_stop_words(test, 'Headline', stop_words)\n",
        "remove_stop_words(test, 'articleBody', stop_words)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rusJy36TZvFs",
        "colab_type": "text"
      },
      "source": [
        "**BERT**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aZrniZZXZtR8",
        "colab_type": "code",
        "outputId": "82bf8d5f-41b7-41d9-b64a-7ad2c3988a71",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 513
        }
      },
      "source": [
        "!pip install pytorch-pretrained-bert"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pytorch-pretrained-bert\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/e0/c08d5553b89973d9a240605b9c12404bcf8227590de62bae27acbcfe076b/pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 2.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (1.9.199)\n",
            "Collecting regex (from pytorch-pretrained-bert)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6f/4e/1b178c38c9a1a184288f72065a65ca01f3154df43c6ad898624149b8b4e0/regex-2019.06.08.tar.gz (651kB)\n",
            "\u001b[K     |████████████████████████████████| 655kB 8.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (2.21.0)\n",
            "Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (1.1.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (4.28.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (1.16.4)\n",
            "Requirement already satisfied: botocore<1.13.0,>=1.12.199 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-bert) (1.12.199)\n",
            "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-bert) (0.2.1)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-bert) (0.9.4)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (1.24.3)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (2019.6.16)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.199->boto3->pytorch-pretrained-bert) (2.5.3)\n",
            "Requirement already satisfied: docutils<0.15,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.199->boto3->pytorch-pretrained-bert) (0.14)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\"->botocore<1.13.0,>=1.12.199->boto3->pytorch-pretrained-bert) (1.12.0)\n",
            "Building wheels for collected packages: regex\n",
            "  Building wheel for regex (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for regex: filename=regex-2019.6.8-cp36-cp36m-linux_x86_64.whl size=604146 sha256=7d39a2d4b4c0d9870ab6ebb1f32ba7e1936782e70300ee529ad8a70735b5cd6b\n",
            "  Stored in directory: /root/.cache/pip/wheels/35/e4/80/abf3b33ba89cf65cd262af8a22a5a999cc28fbfabea6b38473\n",
            "Successfully built regex\n",
            "Installing collected packages: regex, pytorch-pretrained-bert\n",
            "Successfully installed pytorch-pretrained-bert-0.6.2 regex-2019.6.8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P458n45_rUF2",
        "colab_type": "code",
        "outputId": "f0ae6c04-0550-4b87-d71a-ae3de9fcf549",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Load pre-trained model tokenizer (vocabulary)\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 231508/231508 [00:00<00:00, 414959.03B/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9kLMiaNbyzg-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# method to ass start and end tokens to each document\n",
        "def add_special_tokens(data, col):\n",
        "  sentence = data[col]\n",
        "  sentence = \"[CLS] \" + sentence + \" [SEP]\"\n",
        "  data[col] = sentence"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vwsv1ot2zsjC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# add special tokens in dataframes for article Headlines and Bodies\n",
        "add_special_tokens(train, 'Headline')\n",
        "add_special_tokens(train, 'articleBody')\n",
        "add_special_tokens(test, 'Headline')\n",
        "add_special_tokens(test, 'articleBody')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CTwRjUfsz56y",
        "colab_type": "code",
        "outputId": "c085ae25-d357-436e-f6e6-117216f0954f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "train.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Headline</th>\n",
              "      <th>Body ID</th>\n",
              "      <th>Stance</th>\n",
              "      <th>articleBody</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[CLS] Police find mass graves with at least '1...</td>\n",
              "      <td>712</td>\n",
              "      <td>unrelated</td>\n",
              "      <td>[CLS] Danny Boyle is directing the untitled fi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[CLS] Seth Rogen to Play Apple’s Steve Wozniak...</td>\n",
              "      <td>712</td>\n",
              "      <td>discuss</td>\n",
              "      <td>[CLS] Danny Boyle is directing the untitled fi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[CLS] Mexico police find mass grave near site ...</td>\n",
              "      <td>712</td>\n",
              "      <td>unrelated</td>\n",
              "      <td>[CLS] Danny Boyle is directing the untitled fi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[CLS] Mexico Says Missing Students Not Found I...</td>\n",
              "      <td>712</td>\n",
              "      <td>unrelated</td>\n",
              "      <td>[CLS] Danny Boyle is directing the untitled fi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[CLS] New iOS 8 bug can delete all of your iCl...</td>\n",
              "      <td>712</td>\n",
              "      <td>unrelated</td>\n",
              "      <td>[CLS] Danny Boyle is directing the untitled fi...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            Headline  ...                                        articleBody\n",
              "0  [CLS] Police find mass graves with at least '1...  ...  [CLS] Danny Boyle is directing the untitled fi...\n",
              "1  [CLS] Seth Rogen to Play Apple’s Steve Wozniak...  ...  [CLS] Danny Boyle is directing the untitled fi...\n",
              "2  [CLS] Mexico police find mass grave near site ...  ...  [CLS] Danny Boyle is directing the untitled fi...\n",
              "3  [CLS] Mexico Says Missing Students Not Found I...  ...  [CLS] Danny Boyle is directing the untitled fi...\n",
              "4  [CLS] New iOS 8 bug can delete all of your iCl...  ...  [CLS] Danny Boyle is directing the untitled fi...\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u9nxvz7f0DeA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# method to ensure tokenized length of each document is less than 512 tokens\n",
        "def tokenize_text(data):\n",
        "  tokenized_heads = []\n",
        "  tokenized_bodies = []\n",
        "  for index, row in data.iterrows(): #loop through all rows in the dataframe\n",
        "    token_list_head = tokenizer.tokenize(row['Headline']) #headline becomes token_list_head\n",
        "    token_list_body = tokenizer.tokenize(row['articleBody']) #article body becomes token_list_body\n",
        "    if len(token_list_head) > 512: # if the length of the token list head is more than 512\n",
        "      sep = token_list_head[-1] #keep the sentence ending value\n",
        "      token_list_head = token_list_head[:510] #reduce size of sequence\n",
        "      token_list_head.append(sep) #add token ending value to the end\n",
        "    if len(token_list_body) > 512: # repeat for token_list_body\n",
        "      sep = token_list_body[-1]\n",
        "      token_list_body = token_list_body[:510]\n",
        "      token_list_body.append(sep)\n",
        "    tokenized_heads.append(token_list_head) #adds sequences to list defined above\n",
        "    tokenized_bodies.append(token_list_body)\n",
        "  return tokenized_heads, tokenized_bodies #return pair of lists"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c4vnBXSKUODS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#tokenize text for all documents in the dataframes\n",
        "tokenized_train_heads, tokenized_train_bodies = tokenize_text(train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CUP6KcHaT1NU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#tokenize text for all documents in the dataframes\n",
        "tokenized_test_heads, tokenized_test_bodies = tokenize_text(test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wHcUyMTJiyr1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# maps each token in the tokenzied data to a specific index\n",
        "def add_indices(tokenized_data):\n",
        "  indexed_tokenis = []\n",
        "  indexed_tokens = [tokenizer.convert_tokens_to_ids(document) for document in tokenized_data] #create list of indices for all tokens in each document\n",
        "  return indexed_tokens"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B7mprfS6kCdn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create index_token list for every document in training set\n",
        "index_tokens_train_heads = add_indices(tokenized_train_heads)\n",
        "index_tokens_train_bodies = add_indices(tokenized_train_bodies)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2RInd7U4Vgxv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create index token list for every document in test set\n",
        "index_tokens_test_heads = add_indices(tokenized_test_heads)\n",
        "index_tokens_test_bodies = add_indices(tokenized_test_bodies)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sCUkJX8ElFBq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for tup in zip(tokenized_train_bodies[251], index_tokens_train_bodies[251]):\n",
        "  print(tup)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sQmoaR6wgguC",
        "colab_type": "code",
        "outputId": "26aac442-2902-406b-f10d-beefbec3a245",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# load pre-trained model (weights)\n",
        "model = BertModel.from_pretrained('bert-base-uncased')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 407873900/407873900 [00:35<00:00, 11347640.26B/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aNyV6D-qqUx9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# retrieves the sentence vectors for each document\n",
        "def get_sentence_vectors(index_token_list, model):\n",
        "  sentence_vectors = []\n",
        "  model.eval()\n",
        "  \n",
        "  #iterates through all documents in the corpus\n",
        "  for index_list in index_token_list:\n",
        "    # required formatting for document input into model, create a list of segement ids the same as each token list\n",
        "    segments_ids = [1] * len(index_list)\n",
        "    # crates tensor using each index list\n",
        "    tokens_tensor = torch.tensor([index_list])\n",
        "    # creates corresponding tensor for segments ids\n",
        "    segments_tensors = torch.tensor([segments_ids])\n",
        "\n",
        "    with torch.no_grad():\n",
        "      encoded_layers, _ = model(tokens_tensor, segments_tensors)\n",
        "    # code below would be used to retrieve single word embeddings for every word in each document\n",
        "    \n",
        "    #token_embeddings = []\n",
        "\n",
        "    # For each token in the sentence\n",
        "    #for token_i in range(len(token_list)):\n",
        "\n",
        "      # Holds 12 layers of hidden states for each token \n",
        "      #hidden_layers = [] \n",
        "\n",
        "      # For each of the 12 layers...\n",
        "      #for layer_i in range(len(encoded_layers)):\n",
        "\n",
        "        # Lookup the vector for `token_i` in `layer_i`\n",
        "        #vec = encoded_layers[layer_i][batch_i][token_i]\n",
        "\n",
        "        #hidden_layers.append(vec)\n",
        "\n",
        "        #token_embeddings.append(hidden_layers)\n",
        "\n",
        "    sentence_embedding = torch.mean(encoded_layers[11], 1)\n",
        "\n",
        "    sentence_vectors.append(sentence_embedding)\n",
        "  \n",
        "  return sentence_vectors"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v9B4tPBNhCQN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# retrieve sentence vectors for the training headlines\n",
        "train_heads_sentence_vectors = get_sentence_vectors(index_tokens_train_heads, model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LVObDkZ3zT-k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# batches for training set\n",
        "index_tokens_train_bodies_1 = index_tokens_train_bodies[:10000]\n",
        "index_tokens_train_bodies_2 = index_tokens_train_bodies[10000:20000]\n",
        "index_tokens_train_bodies_3 = index_tokens_train_bodies[20000:30000]\n",
        "index_tokens_train_bodies_4 = index_tokens_train_bodies[30000:40000]\n",
        "index_tokens_train_bodies_5 = index_tokens_train_bodies[40000:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fPmCBMgtV5ji",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# batches for test set\n",
        "index_tokens_test_bodies_1 = index_tokens_test_bodies[:10000]\n",
        "index_tokens_test_bodies_2 = index_tokens_test_bodies[10000:20000]\n",
        "index_tokens_test_bodies_3 = index_tokens_test_bodies[20000:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xe8Tez7mYtLf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# saving batches of training set\n",
        "np.save('gdrive/My Drive/Colab Notebooks/MSC_project_data/train_heads_sentence_vectors.npy', heads_sentence_vectors)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cbo8pFRy8TXH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# save corresponding sentence vectors\n",
        "train_bodies_sentence_vectors_1 = get_sentence_vectors(index_tokens_train_bodies_1, model)\n",
        "bodies_sentence_vectors_1 = [t.numpy() for t in train_bodies_sentence_vectors_1]\n",
        "np.save('gdrive/My Drive/Colab Notebooks/MSC_project_data/train_bodies_sentence_vectors_1.npy', bodies_sentence_vectors_1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3jhyQXgk1J3R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bodies_sentence_vectors_2 = get_sentence_vectors(index_tokens_train_bodies_2, model)\n",
        "train_bodies_sentence_vectors_2 = [t.numpy() for t in bodies_sentence_vectors_2]\n",
        "np.save('gdrive/My Drive/Colab Notebooks/MSC_project_data/train_bodies_sentence_vectors_2.npy', train_bodies_sentence_vectors_2)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x8wx61s_3d-c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bodies_sentence_vectors_3 = get_sentence_vectors(index_tokens_train_bodies_3, model)\n",
        "train_bodies_sentence_vectors_3 = [t.numpy() for t in bodies_sentence_vectors_3]\n",
        "np.save('gdrive/My Drive/Colab Notebooks/MSC_project_data/train_bodies_sentence_vectors_3.npy', train_bodies_sentence_vectors_3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dbJIRgXLIMS9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bodies_sentence_vectors_4 = get_sentence_vectors(index_tokens_train_bodies_4, model)\n",
        "train_bodies_sentence_vectors_4 = [t.numpy() for t in bodies_sentence_vectors_4]\n",
        "np.save('gdrive/My Drive/Colab Notebooks/MSC_project_data/train_bodies_sentence_vectors_4.npy', train_bodies_sentence_vectors_4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KvMUtQvPIZRJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bodies_sentence_vectors_5 = get_sentence_vectors(index_tokens_train_bodies_5, model)\n",
        "train_bodies_sentence_vectors_5 = [t.numpy() for t in bodies_sentence_vectors_5]\n",
        "np.save('gdrive/My Drive/Colab Notebooks/MSC_project_data/train_bodies_sentence_vectors_5.npy', train_bodies_sentence_vectors_5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mkLCvDiXTIIC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_bodies_sentence_vectors_1 = get_sentence_vectors(index_tokens_test_bodies_1, model)\n",
        "test_bodies_sentence_vectors_1 = [t.numpy() for t in test_bodies_sentence_vectors_1]\n",
        "np.save('gdrive/My Drive/Colab Notebooks/MSC_project_data/test_bodies_sentence_vectors_1.npy', test_bodies_sentence_vectors_1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Jrt4ur1GgWK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_bodies_sentence_vectors_2 = get_sentence_vectors(index_tokens_test_bodies_2, model)\n",
        "test_bodies_sentence_vectors_2 = [t.numpy() for t in test_bodies_sentence_vectors_2]\n",
        "np.save('gdrive/My Drive/Colab Notebooks/MSC_project_data/test_bodies_sentence_vectors_2.npy', test_bodies_sentence_vectors_2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5nPeiULj389E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_bodies_sentence_vectors_3 = get_sentence_vectors(index_tokens_test_bodies_3, model)\n",
        "test_bodies_sentence_vectors_3 = [t.numpy() for t in test_bodies_sentence_vectors_3]\n",
        "np.save('gdrive/My Drive/Colab Notebooks/MSC_project_data/test_bodies_sentence_vectors_3.npy', test_bodies_sentence_vectors_3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wxBUt94gSle9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_heads_sentence_vectors = get_sentence_vectors(index_tokens_test_heads, model)\n",
        "test_heads_sentence_vectors = [t.numpy() for t in test_heads_sentence_vectors]\n",
        "np.save('gdrive/My Drive/Colab Notebooks/MSC_project_data/test_heads_sentence_vectors.npy', test_heads_sentence_vectors)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C3jGwKzXtzaG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load in sentence vectors\n",
        "train_heads_sentence_vectors = np.load('gdrive/My Drive/Colab Notebooks/MSC_project_data/train_heads_sentence_vectors.npy')\n",
        "\n",
        "test_heads_sentence_vectors = np.load('gdrive/My Drive/Colab Notebooks/MSC_project_data/test_heads_sentence_vectors.npy')\n",
        "\n",
        "train_bodies_sentence_vectors_1 = np.load('gdrive/My Drive/Colab Notebooks/MSC_project_data/train_bodies_sentence_vectors_1.npy')\n",
        "train_bodies_sentence_vectors_2 = np.load('gdrive/My Drive/Colab Notebooks/MSC_project_data/train_bodies_sentence_vectors_2.npy')\n",
        "train_bodies_sentence_vectors_3 = np.load('gdrive/My Drive/Colab Notebooks/MSC_project_data/train_bodies_sentence_vectors_3.npy')\n",
        "train_bodies_sentence_vectors_4 = np.load('gdrive/My Drive/Colab Notebooks/MSC_project_data/train_bodies_sentence_vectors_4.npy')\n",
        "train_bodies_sentence_vectors_5 = np.load('gdrive/My Drive/Colab Notebooks/MSC_project_data/train_bodies_sentence_vectors_5.npy')\n",
        "\n",
        "test_bodies_sentence_vectors_1 = np.load('gdrive/My Drive/Colab Notebooks/MSC_project_data/test_bodies_sentence_vectors_1.npy')\n",
        "test_bodies_sentence_vectors_2 = np.load('gdrive/My Drive/Colab Notebooks/MSC_project_data/test_bodies_sentence_vectors_2.npy')\n",
        "test_bodies_sentence_vectors_3 = np.load('gdrive/My Drive/Colab Notebooks/MSC_project_data/test_bodies_sentence_vectors_3.npy')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zsU4trKTv-ei",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# method to reduce dimension of sentence vectors\n",
        "def reduce_dimension(array):\n",
        "  return np.squeeze(array)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Y51QHlJxXmd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_heads_sentence_vectors = reduce_dimension(train_heads_sentence_vectors)\n",
        "test_heads_sentence_vectors = reduce_dimension(test_heads_sentence_vectors)\n",
        "\n",
        "train_bodies_sentence_vectors_1 = reduce_dimension(train_bodies_sentence_vectors_1)\n",
        "train_bodies_sentence_vectors_2 = reduce_dimension(train_bodies_sentence_vectors_2)\n",
        "train_bodies_sentence_vectors_3 = reduce_dimension(train_bodies_sentence_vectors_3)\n",
        "train_bodies_sentence_vectors_4 = reduce_dimension(train_bodies_sentence_vectors_4)\n",
        "train_bodies_sentence_vectors_5 = reduce_dimension(train_bodies_sentence_vectors_5)\n",
        "\n",
        "test_bodies_sentence_vectors_1 = reduce_dimension(test_bodies_sentence_vectors_1)\n",
        "test_bodies_sentence_vectors_2 = reduce_dimension(test_bodies_sentence_vectors_2)\n",
        "test_bodies_sentence_vectors_3 = reduce_dimension(test_bodies_sentence_vectors_3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LxTL6DNeyQPC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# concatenate all training data\n",
        "train_bodies = np.concatenate((train_bodies_sentence_vectors_1, train_bodies_sentence_vectors_2, train_bodies_sentence_vectors_3,\n",
        "                              train_bodies_sentence_vectors_4, train_bodies_sentence_vectors_5))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fwOFYSS7zHkE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# concatenate all test data\n",
        "test_bodies = np.concatenate((test_bodies_sentence_vectors_1, test_bodies_sentence_vectors_2, test_bodies_sentence_vectors_3))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MSimOQpwzXb2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# re-save files back to google drive\n",
        "np.save('gdrive/My Drive/Colab Notebooks/MSC_project_data/bert_train_heads.npy', train_heads_sentence_vectors)\n",
        "np.save('gdrive/My Drive/Colab Notebooks/MSC_project_data/bert_train_bodies.npy', train_bodies)\n",
        "np.save('gdrive/My Drive/Colab Notebooks/MSC_project_data/bert_test_heads.npy', test_heads_sentence_vectors)\n",
        "np.save('gdrive/My Drive/Colab Notebooks/MSC_project_data/bert_test_bodies.npy', test_bodies)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1oS5S3Pu0n7A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bert_train_heads = np.load('gdrive/My Drive/Colab Notebooks/MSC_project_data/bert_train_heads.npy')\n",
        "bert_train_bodies = np.load('gdrive/My Drive/Colab Notebooks/MSC_project_data/bert_train_bodies.npy')\n",
        "bert_test_heads = np.load('gdrive/My Drive/Colab Notebooks/MSC_project_data/bert_test_heads.npy')\n",
        "bert_test_bodies = np.load('gdrive/My Drive/Colab Notebooks/MSC_project_data/bert_test_bodies.npy')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}